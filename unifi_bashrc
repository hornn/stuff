#export PATH=/usr/local/nginx/sbin:$PATH
alias startpg="sudo /etc/init.d/postgresql start"
alias stoppg="sudo /etc/init.d/postgresql stop"
export JAVA_HOME=/home/nhorn/.sdkman/candidates/java/current/
#PATH=/usr/local/node/bin:$PATH
export PATH=/usr/local/Python/bin:$PATH
#export PATH=/usr/local/redis/bin:$PATH
#export PATH=/usr/local/pgsql/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/pgsql/lib:$LD_LIBRARY_PATH
#export PATH=/usr/local/solr/bin:$PATH
#PATH=/usr/local/hadoop/bin:/usr/local/hadoop/sbin:$PATH
export HADOOP_HOME=/usr/local/hadoop
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
#export PATH=/usr/local/hive/bin:$PATH
export HIVE_HOME=/usr/local/hive
export HIVE_CONF_DIR=$HIVE_HOME/conf
export HADOOP_CLASSPATH=$HIVE_HOME/lib/*:$HIVE_CONF_DIR/*
#export PATH=/usr/local/sqoop/bin:$PATH
export SQOOP_HOME=/usr/local/sqoop
#export PATH=/usr/local/spark/bin:$PATH
export SPARK_HOME=/usr/local/spark
export SPARK_DIST_CLASSPATH=$(hadoop classpath):$HIVE_HOME/lib/*

# /home/cpedrotti/dev/unifi_virtualenv/bin:/usr/local/hadoop/bin:/usr/local/hive/bin:/usr/local/sqoop/bin:/usr/local/redis-4.0.14/bin:/usr/local/spark/bin:/usr/local/solr-7.5.0/bin:/usr/local/nginx/sbin/:/usr/local/node-v12.13.0-linux-x64/bin:/usr/local/pgsql/bin:/home/cpedrotti/Applications/p4v-2020.1.1946989/bin/:/home/cpedrotti/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin

export PATH=/usr/local/redis/bin:/usr/local/nginx/sbin:/usr/local/node/bin:/usr/local/solr/bin:/usr/local/pgsql/bin:$PATH
export PATH=/usr/local/hadoop/bin:/usr/local/hadoop/sbin:/usr/local/hive/bin:/usr/local/sqoop/bin:/usr/local/spark/bin:$PATH

# do not activate by default - use activate command
#. ~/dev/unifi_virtualenv/bin/activate
export UNIFI_VIRT_ENV=/home/nhorn/dev/unifi_virtualenv

### Unifi ###
# active virtual env:
alias activate='source ~/dev/unifi_virtualenv/bin/activate'
alias activate3='source ~/dev/unifi_virtualenv3/bin/activate'
# start/stop postgres
alias start_postgres='pg_ctl -D /usr/local/pgsql/data -l /usr/local/pgsql/server.log start'
alias stop_postgres='pg_ctl -D /usr/local/pgsql/data stop'
# start/stop hdfs
alias start_hdfs='/usr/local/hadoop/sbin/start-dfs.sh'
alias stop_hdfs='/usr/local/hadoop/sbin/stop-dfs.sh'
# start/stop yarn
alias start_yarn='/usr/local/hadoop/sbin/start-yarn.sh'
alias stop_yarn='/usr/local/hadoop/sbin/stop-yarn.sh'
# start hive
# hive 1.2.x/on mac
# alias start_hive='hiveserver2 &'
alias start_hive='start-hive.sh'
# stop hive
function stop_hive()
{
	echo "killing hiveserver2..."
	ps -ef | grep HiveServer2 | awk '{print $2}' | xargs kill -9
	echo "killing hive metastore..."
	ps -ef | grep HiveMetaStore | awk '{print $2}' | xargs kill -9 
}

# start all (postgres + hdfs + yarn + hiveserver2)
function start_all()
{
	startpg
	start_hdfs
	hdfs dfsadmin -safemode wait
	start_yarn
	start_hive
}
function stop_all()
{
	stoppg
	stop_hdfs
	stop_yarn
	stop_hive
}

# install, start, stop unifi
#alias python='python -3'
alias unifi_start='/home/nhorn/dev/unifing/scripts/sbin/unifi_start --executor-mode local'
alias unifi_status='/home/nhorn/dev/unifing/scripts/sbin/unifi_status'
alias unifi_stop='/home/nhorn/dev/unifing/scripts/sbin/unifi_stop'
alias unifi_restart='/home/nhorn/dev/unifing/scripts/sbin/unifi_restart --executor-mode local'
function unifi_restart_old() {
  stop_params=''
  stop_list='--all --celery --solr --discovery --executor --access --integration --redis'
  for var in "$@"; do
    if [[ $stop_list =~ $var ]]; then
      stop_params="${stop_params}${var}"
    fi;
  done;
  echo "stop params: $stop_params";
  unifi_stop $stop_params;
  echo "start params: $@";
  unifi_start $@;
}

alias unifi_build='unifi_stop && gotobase; ./gradlew clean && ./gradlew build --parallel --max-workers=4 -Pfastpack=true; cd -; bt'
alias unifi_build_offline='unifi_stop && gradlew clean && gradlew build --parallel --max-workers=4 -Pfastpack=true --offline; bt'
alias unifi_clean_cache='cd /home/nhorn/dev/unifing/services/data-integration/unifi_www/datai/static/angular/; rm -r .cache/ node_modules/ dist/ package-lock.json; cd -'
alias unifi_install_default='rm /home/nhorn/dev/unifing/services/data-integration/unifi_www/unifi_www/settings.py ; rm /home/nhorn/dev/unifing/ext/redis/3.0.1/dump.rdb ; /home/nhorn/dev/unifing/scripts/sbin/unifi_install --dbhost=127.0.0.1 --dbport=5432 --dbuser=unifi --dbpass='\'''\'' --dbname=unifi --unifiuser=unifi --unifipass=unifiU1! --unifiemail=noa@unifisoftware.com --unififirstname Noa --unifilastname Horn --install-missing-deps'
alias unifi_install_demo='cd /home/nhorn/dev/unifing ; python -m demo.setup --unifiuser=unifi --unifipass=unifiU1! --all; cd -'
#alias unifi_install_demo='cd /home/nhorn/dev/unifing/demo ; python setup.py --unifiuser=unifi --unifipass=unifiU1! --db --hdfs --s3 --azure-blob --gs --service; cd -'
alias unifi_reinstall='unifi_stop ; dropdb unifi ; unifi_install_default ; unifi_start ; unifi_install_demo'
alias gotoapi='cd /home/nhorn/dev/unifing/services/data-integration/unifi_www/'
alias gotobase='cd /home/nhorn/dev/unifing/'
alias gototest='cd /home/nhorn/dev/unifing/test/'
function unifi_test()
{
if [[ "$1" == "" ]]; then
    echo "Usage: unifi_test <keyword>"
    return 1
fi
keyword=$1
gototest
(set -x; python regression.py --keyword $keyword --pytest-args='-s' --keep-changes --extended)
cd -
}
alias unifi_log='/home/nhorn/dev/unifing/scripts/sbin/unifi_log'
alias curlaccess="psql unifi -c \"select key from authtoken_token\" -t | xargs -I % echo \"curl -H \\\"Authorization: Token\" %\\\""



